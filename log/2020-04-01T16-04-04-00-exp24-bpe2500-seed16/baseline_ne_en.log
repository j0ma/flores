================ FLORES BASELINE WITH CLIP_NORM=0.1, BPE=2500 AND SEED=16  ================
About to train the supervised for the following language pair: NE-EN
Checkpoint directory unset! Setting to default value...
CHECKPOINT_DIR is set to './checkpoints/2020-04-01T16-04-04-00-exp24-bpe2500-seed16/checkpoints_ne_en'
Creating checkpoint directory if it doesn't exist...
Data folder is: data-bin/wiki_ne_en_bpe2500/
Beginning training...
Time at beginning: Wed Apr 1 16:04:24 EDT 2020
Namespace(activation_dropout=0.2, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.2, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.1, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/wiki_ne_en_bpe2500/', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=2, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=5, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.4, empty_cache_freq=0, encoder_attention_heads=2, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=5, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.2, layer_wise_attention=False, layernorm_embedding=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='inverse_sqrt', max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/2020-04-01T16-04-04-00-exp24-bpe2500-seed16/checkpoints_ne_en', save_interval=10, save_interval_updates=0, seed=16, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='ne', target_lang='en', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[4], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001)
| [ne] dictionary: 2504 types
| [en] dictionary: 2504 types
| loaded 2559 examples from: data-bin/wiki_ne_en_bpe2500/valid.ne-en.ne
| loaded 2559 examples from: data-bin/wiki_ne_en_bpe2500/valid.ne-en.en
| data-bin/wiki_ne_en_bpe2500/ valid ne-en 2559 examples
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(2504, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(2504, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
| model transformer, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 38066176 (num. trained: 38066176)
| training on 1 GPUs
| max tokens per GPU = 4000 and max sentences per GPU = None
| no existing checkpoint found ./checkpoints/2020-04-01T16-04-04-00-exp24-bpe2500-seed16/checkpoints_ne_en/checkpoint_last.pt
| loading train data for epoch 0
| loaded 563637 examples from: data-bin/wiki_ne_en_bpe2500/train.ne-en.ne
| loaded 563637 examples from: data-bin/wiki_ne_en_bpe2500/train.ne-en.en
| data-bin/wiki_ne_en_bpe2500/ train ne-en 563637 examples
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| WARNING: overflow detected, setting loss scale to: 16.0
| epoch 001 | loss 9.196 | nll_loss 8.301 | ppl 315.3 | wps 52305 | ups 4 | wpb 12801.670 | bsz 809.836 | num_updates 691 | lr 0.000172833 | gnorm 0.863 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 169 | train_wall 132
| epoch 001 | valid on 'valid' subset | loss 8.386 | nll_loss 7.054 | ppl 132.93 | num_updates 691
| epoch 002 | loss 7.186 | nll_loss 5.613 | ppl 48.93 | wps 53341 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 1385 | lr 0.000346315 | gnorm 0.582 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 337 | train_wall 266
| epoch 002 | valid on 'valid' subset | loss 7.855 | nll_loss 6.273 | ppl 77.34 | num_updates 1385
| epoch 003 | loss 6.388 | nll_loss 4.562 | ppl 23.62 | wps 54010 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 2079 | lr 0.000519798 | gnorm 0.493 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 502 | train_wall 400
| epoch 003 | valid on 'valid' subset | loss 7.561 | nll_loss 5.878 | ppl 58.79 | num_updates 2079
| epoch 004 | loss 5.930 | nll_loss 3.964 | ppl 15.61 | wps 53833 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 2773 | lr 0.000693281 | gnorm 0.437 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 668 | train_wall 532
| epoch 004 | valid on 'valid' subset | loss 7.402 | nll_loss 5.623 | ppl 49.3 | num_updates 2773
| epoch 005 | loss 5.633 | nll_loss 3.581 | ppl 11.97 | wps 55235 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 3467 | lr 0.000866763 | gnorm 0.384 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 830 | train_wall 661
| epoch 005 | valid on 'valid' subset | loss 7.091 | nll_loss 5.251 | ppl 38.09 | num_updates 3467
| epoch 006 | loss 5.443 | nll_loss 3.339 | ppl 10.12 | wps 53364 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 4161 | lr 0.000980463 | gnorm 0.345 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 997 | train_wall 796
| epoch 006 | valid on 'valid' subset | loss 7.007 | nll_loss 5.137 | ppl 35.2 | num_updates 4161
| epoch 007 | loss 5.274 | nll_loss 3.126 | ppl 8.73 | wps 52837 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 4855 | lr 0.000907685 | gnorm 0.308 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 1166 | train_wall 932
| epoch 007 | valid on 'valid' subset | loss 6.871 | nll_loss 4.937 | ppl 30.64 | num_updates 4855
| WARNING: overflow detected, setting loss scale to: 16.0
| epoch 008 | loss 5.138 | nll_loss 2.954 | ppl 7.75 | wps 53495 | ups 4 | wpb 12802.945 | bsz 809.173 | num_updates 5548 | lr 0.000849106 | gnorm 0.282 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 1333 | train_wall 1067
| epoch 008 | valid on 'valid' subset | loss 6.706 | nll_loss 4.707 | ppl 26.12 | num_updates 5548
| epoch 009 | loss 5.035 | nll_loss 2.824 | ppl 7.08 | wps 54586 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 6242 | lr 0.000800512 | gnorm 0.267 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 1496 | train_wall 1199
| epoch 009 | valid on 'valid' subset | loss 6.665 | nll_loss 4.652 | ppl 25.14 | num_updates 6242
| epoch 010 | loss 4.954 | nll_loss 2.723 | ppl 6.6 | wps 53838 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 6936 | lr 0.000759408 | gnorm 0.252 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 1662 | train_wall 1332
| epoch 010 | valid on 'valid' subset | loss 6.618 | nll_loss 4.585 | ppl 24 | num_updates 6936
| saved checkpoint ./checkpoints/2020-04-01T16-04-04-00-exp24-bpe2500-seed16/checkpoints_ne_en/checkpoint10.pt (epoch 10 @ 6936 updates) (writing took 1.0131394863128662 seconds)
| epoch 011 | loss 4.893 | nll_loss 2.645 | ppl 6.26 | wps 56603 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 7630 | lr 0.000724049 | gnorm 0.248 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 1821 | train_wall 1460
| epoch 011 | valid on 'valid' subset | loss 6.554 | nll_loss 4.507 | ppl 22.74 | num_updates 7630 | best_loss 6.55427
| epoch 012 | loss 4.838 | nll_loss 2.577 | ppl 5.97 | wps 55352 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 8324 | lr 0.000693209 | gnorm 0.240 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 1983 | train_wall 1589
| epoch 012 | valid on 'valid' subset | loss 6.480 | nll_loss 4.402 | ppl 21.14 | num_updates 8324 | best_loss 6.48002
| epoch 013 | loss 4.794 | nll_loss 2.522 | ppl 5.74 | wps 53720 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 9018 | lr 0.000666001 | gnorm 0.236 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 2149 | train_wall 1723
| epoch 013 | valid on 'valid' subset | loss 6.492 | nll_loss 4.422 | ppl 21.44 | num_updates 9018 | best_loss 6.49173
| epoch 014 | loss 4.755 | nll_loss 2.473 | ppl 5.55 | wps 54598 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 9712 | lr 0.000641764 | gnorm 0.229 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2312 | train_wall 1855
| epoch 014 | valid on 'valid' subset | loss 6.456 | nll_loss 4.376 | ppl 20.77 | num_updates 9712 | best_loss 6.45632
| epoch 015 | loss 4.721 | nll_loss 2.430 | ppl 5.39 | wps 59503 | ups 5 | wpb 12806.207 | bsz 812.157 | num_updates 10406 | lr 0.000619995 | gnorm 0.228 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2462 | train_wall 1976
| epoch 015 | valid on 'valid' subset | loss 6.395 | nll_loss 4.298 | ppl 19.67 | num_updates 10406 | best_loss 6.39543
| epoch 016 | loss 4.691 | nll_loss 2.392 | ppl 5.25 | wps 59490 | ups 5 | wpb 12806.207 | bsz 812.157 | num_updates 11100 | lr 0.0006003 | gnorm 0.226 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2613 | train_wall 2097
| epoch 016 | valid on 'valid' subset | loss 6.408 | nll_loss 4.306 | ppl 19.78 | num_updates 11100 | best_loss 6.40795
| epoch 017 | loss 4.664 | nll_loss 2.359 | ppl 5.13 | wps 57863 | ups 5 | wpb 12806.207 | bsz 812.157 | num_updates 11794 | lr 0.000582371 | gnorm 0.224 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2767 | train_wall 2221
| epoch 017 | valid on 'valid' subset | loss 6.372 | nll_loss 4.274 | ppl 19.35 | num_updates 11794 | best_loss 6.37163
| epoch 018 | loss 4.640 | nll_loss 2.329 | ppl 5.02 | wps 53485 | ups 4 | wpb 12806.207 | bsz 812.157 | num_updates 12488 | lr 0.000565957 | gnorm 0.222 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2934 | train_wall 2355
