================ FLORES BASELINE WITH CLIP_NORM=0.1, BPE=7500 AND SEED=15  ================
About to train the supervised for the following language pair: SI-EN
Checkpoint directory unset! Setting to default value...
CHECKPOINT_DIR is set to './checkpoints/2020-04-09T01-03-04-00-exp25-bpe7500-seed15/checkpoints_si_en'
Creating checkpoint directory if it doesn't exist...
Data folder is: data-bin/wiki_si_en_bpe7500/
Beginning training...
Time at beginning: Thu Apr 9 07:55:38 EDT 2020
Namespace(activation_dropout=0.2, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.2, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.1, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/wiki_si_en_bpe7500/', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=2, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=5, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.4, empty_cache_freq=0, encoder_attention_heads=2, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=5, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.2, layer_wise_attention=False, layernorm_embedding=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='inverse_sqrt', max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/2020-04-09T01-03-04-00-exp25-bpe7500-seed15/checkpoints_si_en', save_interval=10, save_interval_updates=0, seed=15, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='si', target_lang='en', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[4], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001)
| [si] dictionary: 7464 types
| [en] dictionary: 7464 types
| loaded 2898 examples from: data-bin/wiki_si_en_bpe7500/valid.si-en.si
| loaded 2898 examples from: data-bin/wiki_si_en_bpe7500/valid.si-en.en
| data-bin/wiki_si_en_bpe7500/ valid si-en 2898 examples
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(7464, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(7464, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
| model transformer, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 40605696 (num. trained: 40605696)
| training on 1 GPUs
| max tokens per GPU = 4000 and max sentences per GPU = None
| no existing checkpoint found ./checkpoints/2020-04-09T01-03-04-00-exp25-bpe7500-seed15/checkpoints_si_en/checkpoint_last.pt
| loading train data for epoch 0
| loaded 385319 examples from: data-bin/wiki_si_en_bpe7500/train.si-en.si
| loaded 385319 examples from: data-bin/wiki_si_en_bpe7500/train.si-en.en
| data-bin/wiki_si_en_bpe7500/ train si-en 385319 examples
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| WARNING: overflow detected, setting loss scale to: 16.0
| epoch 001 | loss 10.400 | nll_loss 9.455 | ppl 701.85 | wps 56878 | ups 4 | wpb 14099.629 | bsz 1019.069 | num_updates 375 | lr 9.38406e-05 | gnorm 0.839 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 93 | train_wall 71
| epoch 001 | valid on 'valid' subset | loss 10.563 | nll_loss 9.627 | ppl 790.71 | num_updates 375
| epoch 002 | loss 8.695 | nll_loss 7.182 | ppl 145.18 | wps 59331 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 753 | lr 0.000188331 | gnorm 0.559 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 183 | train_wall 142
| epoch 002 | valid on 'valid' subset | loss 9.733 | nll_loss 8.477 | ppl 356.29 | num_updates 753
| epoch 003 | loss 8.035 | nll_loss 6.302 | ppl 78.93 | wps 59336 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 1131 | lr 0.000282822 | gnorm 0.456 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 274 | train_wall 212
| epoch 003 | valid on 'valid' subset | loss 9.221 | nll_loss 7.724 | ppl 211.41 | num_updates 1131
| epoch 004 | loss 7.545 | nll_loss 5.654 | ppl 50.36 | wps 60042 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 1509 | lr 0.000377312 | gnorm 0.418 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 363 | train_wall 282
| epoch 004 | valid on 'valid' subset | loss 8.835 | nll_loss 7.150 | ppl 141.99 | num_updates 1509
| epoch 005 | loss 7.190 | nll_loss 5.184 | ppl 36.36 | wps 59210 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 1887 | lr 0.000471803 | gnorm 0.392 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 454 | train_wall 353
| epoch 005 | valid on 'valid' subset | loss 8.561 | nll_loss 6.770 | ppl 109.15 | num_updates 1887
| epoch 006 | loss 6.924 | nll_loss 4.833 | ppl 28.5 | wps 61220 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 2265 | lr 0.000566293 | gnorm 0.365 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 542 | train_wall 422
| epoch 006 | valid on 'valid' subset | loss 8.319 | nll_loss 6.406 | ppl 84.8 | num_updates 2265
| epoch 007 | loss 6.715 | nll_loss 4.559 | ppl 23.57 | wps 60752 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 2643 | lr 0.000660784 | gnorm 0.351 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 630 | train_wall 491
| epoch 007 | valid on 'valid' subset | loss 8.144 | nll_loss 6.181 | ppl 72.57 | num_updates 2643
| epoch 008 | loss 6.555 | nll_loss 4.351 | ppl 20.41 | wps 59736 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 3021 | lr 0.000755274 | gnorm 0.341 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 720 | train_wall 561
| epoch 008 | valid on 'valid' subset | loss 7.959 | nll_loss 5.940 | ppl 61.4 | num_updates 3021
| epoch 009 | loss 6.439 | nll_loss 4.200 | ppl 18.37 | wps 61058 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 3399 | lr 0.000849765 | gnorm 0.327 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 808 | train_wall 630
| epoch 009 | valid on 'valid' subset | loss 7.857 | nll_loss 5.801 | ppl 55.75 | num_updates 3399
| epoch 010 | loss 6.346 | nll_loss 4.081 | ppl 16.92 | wps 60493 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 3777 | lr 0.000944256 | gnorm 0.309 | clip 1.000 | oom 0.000 | loss_scale 16.000 | wall 897 | train_wall 700
| epoch 010 | valid on 'valid' subset | loss 7.776 | nll_loss 5.680 | ppl 51.28 | num_updates 3777
| saved checkpoint ./checkpoints/2020-04-09T01-03-04-00-exp25-bpe7500-seed15/checkpoints_si_en/checkpoint10.pt (epoch 10 @ 3777 updates) (writing took 0.7390787601470947 seconds)
| epoch 011 | loss 6.272 | nll_loss 3.985 | ppl 15.84 | wps 60482 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 4155 | lr 0.00098117 | gnorm 0.291 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 987 | train_wall 769
| epoch 011 | valid on 'valid' subset | loss 7.686 | nll_loss 5.540 | ppl 46.53 | num_updates 4155 | best_loss 7.68643
| epoch 012 | loss 6.195 | nll_loss 3.886 | ppl 14.79 | wps 60107 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 4533 | lr 0.000939371 | gnorm 0.275 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 1076 | train_wall 839
| epoch 012 | valid on 'valid' subset | loss 7.603 | nll_loss 5.414 | ppl 42.63 | num_updates 4533 | best_loss 7.60275
| epoch 013 | loss 6.122 | nll_loss 3.794 | ppl 13.87 | wps 60405 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 4911 | lr 0.000902495 | gnorm 0.258 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 1165 | train_wall 908
| epoch 013 | valid on 'valid' subset | loss 7.533 | nll_loss 5.338 | ppl 40.46 | num_updates 4911 | best_loss 7.53291
| epoch 014 | loss 6.065 | nll_loss 3.720 | ppl 13.18 | wps 60709 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 5289 | lr 0.000869647 | gnorm 0.251 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 1254 | train_wall 977
| epoch 014 | valid on 'valid' subset | loss 7.446 | nll_loss 5.205 | ppl 36.88 | num_updates 5289 | best_loss 7.44584
| epoch 015 | loss 6.012 | nll_loss 3.653 | ppl 12.58 | wps 59708 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 5667 | lr 0.000840143 | gnorm 0.234 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 1344 | train_wall 1047
| epoch 015 | valid on 'valid' subset | loss 7.390 | nll_loss 5.121 | ppl 34.81 | num_updates 5667 | best_loss 7.38966
| epoch 016 | loss 5.971 | nll_loss 3.600 | ppl 12.13 | wps 60080 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 6045 | lr 0.000813452 | gnorm 0.233 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 1433 | train_wall 1117
| epoch 016 | valid on 'valid' subset | loss 7.336 | nll_loss 5.035 | ppl 32.8 | num_updates 6045 | best_loss 7.33581
| epoch 017 | loss 5.932 | nll_loss 3.551 | ppl 11.72 | wps 60631 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 6423 | lr 0.000789153 | gnorm 0.228 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 1522 | train_wall 1186
| epoch 017 | valid on 'valid' subset | loss 7.280 | nll_loss 4.994 | ppl 31.87 | num_updates 6423 | best_loss 7.28007
| epoch 018 | loss 5.899 | nll_loss 3.509 | ppl 11.38 | wps 61528 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 6801 | lr 0.000766909 | gnorm 0.222 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 1609 | train_wall 1255
| epoch 018 | valid on 'valid' subset | loss 7.235 | nll_loss 4.910 | ppl 30.06 | num_updates 6801 | best_loss 7.2351
| epoch 019 | loss 5.869 | nll_loss 3.471 | ppl 11.09 | wps 60444 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 7179 | lr 0.000746445 | gnorm 0.218 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 1698 | train_wall 1325
| epoch 019 | valid on 'valid' subset | loss 7.250 | nll_loss 4.934 | ppl 30.56 | num_updates 7179 | best_loss 7.24991
| epoch 020 | loss 5.842 | nll_loss 3.436 | ppl 10.83 | wps 60486 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 7557 | lr 0.000727537 | gnorm 0.215 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 1787 | train_wall 1395
| epoch 020 | valid on 'valid' subset | loss 7.191 | nll_loss 4.855 | ppl 28.93 | num_updates 7557 | best_loss 7.19099
| saved checkpoint ./checkpoints/2020-04-09T01-03-04-00-exp25-bpe7500-seed15/checkpoints_si_en/checkpoint20.pt (epoch 20 @ 7557 updates) (writing took 1.2499167919158936 seconds)
| epoch 021 | loss 5.817 | nll_loss 3.405 | ppl 10.59 | wps 59850 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 7935 | lr 0.000709997 | gnorm 0.211 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 1878 | train_wall 1465
| epoch 021 | valid on 'valid' subset | loss 7.146 | nll_loss 4.785 | ppl 27.57 | num_updates 7935 | best_loss 7.14626
| WARNING: overflow detected, setting loss scale to: 32.0
| epoch 022 | loss 5.795 | nll_loss 3.377 | ppl 10.39 | wps 59273 | ups 4 | wpb 14100.162 | bsz 1020.008 | num_updates 8312 | lr 0.000693709 | gnorm 0.212 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 1968 | train_wall 1536
| epoch 022 | valid on 'valid' subset | loss 7.136 | nll_loss 4.759 | ppl 27.08 | num_updates 8312 | best_loss 7.13587
| epoch 023 | loss 5.775 | nll_loss 3.351 | ppl 10.2 | wps 61018 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 8690 | lr 0.000678454 | gnorm 0.206 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2056 | train_wall 1605
| epoch 023 | valid on 'valid' subset | loss 7.111 | nll_loss 4.730 | ppl 26.54 | num_updates 8690 | best_loss 7.11069
| epoch 024 | loss 5.757 | nll_loss 3.328 | ppl 10.04 | wps 60116 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 9068 | lr 0.000664162 | gnorm 0.207 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2145 | train_wall 1675
| epoch 024 | valid on 'valid' subset | loss 7.123 | nll_loss 4.741 | ppl 26.75 | num_updates 9068 | best_loss 7.12293
| epoch 025 | loss 5.739 | nll_loss 3.306 | ppl 9.89 | wps 60918 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 9446 | lr 0.000650738 | gnorm 0.206 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2234 | train_wall 1744
| epoch 025 | valid on 'valid' subset | loss 7.076 | nll_loss 4.685 | ppl 25.72 | num_updates 9446 | best_loss 7.0764
| epoch 026 | loss 5.722 | nll_loss 3.283 | ppl 9.74 | wps 60485 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 9824 | lr 0.000638096 | gnorm 0.203 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2323 | train_wall 1813
| epoch 026 | valid on 'valid' subset | loss 7.070 | nll_loss 4.663 | ppl 25.34 | num_updates 9824 | best_loss 7.06959
| epoch 027 | loss 5.707 | nll_loss 3.265 | ppl 9.61 | wps 60744 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 10202 | lr 0.000626163 | gnorm 0.202 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2411 | train_wall 1883
| epoch 027 | valid on 'valid' subset | loss 7.074 | nll_loss 4.680 | ppl 25.64 | num_updates 10202 | best_loss 7.07401
| epoch 028 | loss 5.693 | nll_loss 3.247 | ppl 9.49 | wps 59570 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 10580 | lr 0.000614875 | gnorm 0.204 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2501 | train_wall 1953
| epoch 028 | valid on 'valid' subset | loss 7.021 | nll_loss 4.605 | ppl 24.34 | num_updates 10580 | best_loss 7.02063
| epoch 029 | loss 5.681 | nll_loss 3.232 | ppl 9.39 | wps 62339 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 10958 | lr 0.000604177 | gnorm 0.201 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2587 | train_wall 2021
| epoch 029 | valid on 'valid' subset | loss 7.061 | nll_loss 4.657 | ppl 25.22 | num_updates 10958 | best_loss 7.06066
| epoch 030 | loss 5.667 | nll_loss 3.213 | ppl 9.27 | wps 61084 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 11336 | lr 0.000594019 | gnorm 0.197 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2675 | train_wall 2090
| epoch 030 | valid on 'valid' subset | loss 7.055 | nll_loss 4.643 | ppl 24.99 | num_updates 11336 | best_loss 7.05459
| saved checkpoint ./checkpoints/2020-04-09T01-03-04-00-exp25-bpe7500-seed15/checkpoints_si_en/checkpoint30.pt (epoch 30 @ 11336 updates) (writing took 1.355421543121338 seconds)
| epoch 031 | loss 5.655 | nll_loss 3.198 | ppl 9.17 | wps 59597 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 11714 | lr 0.000584356 | gnorm 0.197 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2767 | train_wall 2160
| epoch 031 | valid on 'valid' subset | loss 6.980 | nll_loss 4.553 | ppl 23.47 | num_updates 11714 | best_loss 6.98013
| epoch 032 | loss 5.643 | nll_loss 3.183 | ppl 9.08 | wps 60433 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 12092 | lr 0.00057515 | gnorm 0.196 | clip 1.000 | oom 0.000 | loss_scale 32.000 | wall 2856 | train_wall 2230
| epoch 032 | valid on 'valid' subset | loss 6.973 | nll_loss 4.552 | ppl 23.45 | num_updates 12092 | best_loss 6.97326
| epoch 033 | loss 5.632 | nll_loss 3.169 | ppl 9 | wps 60552 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 12470 | lr 0.000566365 | gnorm 0.197 | clip 1.000 | oom 0.000 | loss_scale 64.000 | wall 2944 | train_wall 2300
| epoch 033 | valid on 'valid' subset | loss 6.982 | nll_loss 4.557 | ppl 23.53 | num_updates 12470 | best_loss 6.98218
| epoch 034 | loss 5.623 | nll_loss 3.157 | ppl 8.92 | wps 60020 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 12848 | lr 0.000557972 | gnorm 0.197 | clip 1.000 | oom 0.000 | loss_scale 64.000 | wall 3034 | train_wall 2370
| epoch 034 | valid on 'valid' subset | loss 6.928 | nll_loss 4.497 | ppl 22.58 | num_updates 12848 | best_loss 6.92844
| epoch 035 | loss 5.614 | nll_loss 3.146 | ppl 8.85 | wps 60473 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 13226 | lr 0.000549941 | gnorm 0.198 | clip 1.000 | oom 0.000 | loss_scale 64.000 | wall 3123 | train_wall 2439
| epoch 035 | valid on 'valid' subset | loss 6.922 | nll_loss 4.471 | ppl 22.18 | num_updates 13226 | best_loss 6.92185
| epoch 036 | loss 5.603 | nll_loss 3.132 | ppl 8.77 | wps 59911 | ups 4 | wpb 14103.735 | bsz 1019.362 | num_updates 13604 | lr 0.000542246 | gnorm 0.195 | clip 1.000 | oom 0.000 | loss_scale 64.000 | wall 3212 | train_wall 2509
| epoch 036 | valid on 'valid' subset | loss 6.961 | nll_loss 4.516 | ppl 22.88 | num_updates 13604 | best_loss 6.96093
